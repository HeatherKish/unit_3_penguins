---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, collapse=TRUE) 
```

### Unit 3: Penguins
#### Lesson 3: t-tests
#### New functions: identify_outliers(), stat_qq(), t.test(), t_test(), kable(), cohens_d(), droplevels(), levene_test()

***

### Intro to t-tests

t-tests are used to assess the difference between two means. When conducting a t-test, you are testing the null hypothesis (H0) that the means are the same. As a standard practice in classical (frequentist) statistics, if the p-value resulting from your t-test is < 0.05, you reject the null hypothesis in favor of the alternative hypothesis, which states that the means are significantly different. 

There are 3 types of t-test:

-  One-sample t-test
-  Independent sample t-test
-  Paired t-test

We'll use the `tidyverse` and our palmer penguins data to run through an example of each of these types of t-test. We'll also load in the package `rstatix` because it contains pipe-friendly statistics functions that play nicely with the `tidyverse.` Don't forget to install the `rstatix` package if it is your first time using it! I'm also going to use the kable() function in the knitr package to print out some of the results tables neatly for this tutorial. This is just for aesthetics, and is not necessary for any of the plotting or statistical tests in this lesson.

```{r, message=FALSE}
library(tidyverse)
library(palmerpenguins)
library(rstatix)
library(knitr)  # prints pretty tables
```

### One-sample t-test

The one-sample t-test, also known as the single-parameter t test or single-sample t-test, is used to compare the mean of one sample to a known standard (or theoretical / hypothetical) mean. Generally, the theoretical mean comes from somewhere in the literature or from a previous experiment in your own lab. 

The one-sample t-test assumes the following characteristics about the data:

-  No significant outliers in the data
-  Data should be approximately normally distributed

#### Exploratory data analysis

Before conducting any statistical analyses on your data, it's imperative that you understand your data. This sounds obvious, but it's a step that people all too often skip. Here are some great goals for exploratory data analysis:

1. Looking at the raw data values.
2. Computing summary statistics.
3. Creating data visualizations.

These are things we've been doing already in the course, but now that we are actually trying to run a statistical test with the penguins data, we should use the functions in base R and the tidyverse to formally run through these steps. We will be looking at the difference in body mass between the three penguin species in our observations: Gentoo, Chinstrap and Adelie.

```{r}
# Two great functions for looking at the first few rows of your variables:
head(penguins)
glimpse(penguins)

# Summary statistics. Note # of observations and NAs
summary(penguins)

ggplot(data=penguins) +
  geom_histogram(aes(x=body_mass_g, fill=species))
```

#### Example: Observed Gentoo body mass vs. literature value

Are the Gentoo penguin body mass observations collected in our palmer penguins dataset significantly different than the mean Gentoo penguin body mass accepted in the literature? We can use the body mass value from the Encyclopedia of Life:

https://eol.org/traitbank

Search for "Gentoo Penguin" in the search bar and you will find that the trait bank lists body mass as 6500g. Let's see what our Gentoo body mass observations look like:

```{r}
# Separate just the Gentoo from all the penguin data
gentoo = penguins %>% 
  filter(species=="Gentoo") 

# Quickly visualize the body mass data
ggplot(data=gentoo) +
  geom_histogram(aes(x=body_mass_g))

# Calculate the mean and standard deviation Gentoo body mass in our data (sometimes base R is more sensible than dplyr)
mean(gentoo$body_mass_g, na.rm=TRUE)
sd(gentoo$body_mass_g, na.rm=TRUE)
```

Before we conduct our one-sample t-test, let's first check the assumptions. Are there any significant outliers in the data? The function identify_outliers() uses boxplot methods to return a data frame of outliers (see `?identify_outliers` for more info)

```{r}
# Test for the presence of outliers in the Gentoo body mass data
gentoo %>%
  identify_outliers(body_mass_g)

# Note: here is a result from a made-up dataset where I added an outlier
# data.frame(dat=c(rnorm(100), 312)) %>% identify_outliers()  
```

The identify_outliers() test returned nothing, so there were no outliers in the Gentoo body_mass_g data. 

The normality assumption can be checked by plotting the data in a Quantile-Quantile plot (QQ plot) and see if it mostly falls along the 1:1 line.

```{r}
# Check normality assumption with a qqplot:
ggplot(gentoo) +
  stat_qq(aes(sample=body_mass_g))
```

If the data are not normally distributed, it’s recommended to use a non-parametric test such as the one-sample Wilcoxon signed-rank test. This test is similar to the one-sample t-test, but focuses on the median rather than the mean.

Now let's do our one-sample t-test to see if our body mass data is significantly different from the body mass value of mu = 6500 g published in the literature:

```{r}
t.test(gentoo$body_mass_g, mu = 5950) # Base R

t_test_results = gentoo %>% t_test(body_mass_g ~ 1, mu = 5950) # dplyr-friendly version
kable(t_test_results)
```
The results of our one-sample t-test show the following components:

-  .y.: the outcome variable used in the test.
-  group1,group2: generally, the compared groups in the pairwise tests. Here, we have null model (one-sample test).
-  statistic: test statistic (t-value) used to compute the p-value.
-  df: degrees of freedom.
-  p: p-value.

The output p-value is much less than 0.05 (it's almost p=0). That means we reject our null hypothesis that our Gentoo body mass observations are similar to the literature value.

To calculate an effect size, called Cohen's d, for the one-sample t-test you need to divide the mean difference by the standard deviation of the difference, as shown below. Note that since mu is a constant: sd(x-mu) = sd(x).

Cohen’s d formula:

d = (mean(x) - mu)/sd(x), where:

-  x is a numeric vector containing the data.
-  mu is the mean against which the mean of x is compared (default value is mu = 0).

```{r}
gentoo %>% cohens_d(body_mass_g ~ 1, mu = 6500)
```

Recall that, t-test conventional effect sizes, proposed by Cohen J. (1998), are: 0.2 (small effect), 0.5 (moderate effect) and 0.8 (large effect) (Cohen 1998). As the effect size, d, is -2.82 you can conclude that there is a large effect, and our sample data is less than the supplied literature value mu (=6500).

Huh. Interesting. Remember earlier we caculated our observed mean Gentoo body mass is 5076 g. Maybe our penguins were hungry. If I had to guess, the Encyclopedia of Life body mass trait is probably for adult Gentoo penguins, and we include juveniles in our observations. Or perhaps the Encylcopedia value is junk. If this were actually my data and I wanted to publish on it, I'd use this as a jumping point to do some more research.

### Independent sample t-test:

The independent samples t-test (or unpaired samples t-test) is used to compare the mean of two independent groups. For example, you might want to compare the average weights of individuals grouped by gender: male and female groups, which are two unrelated/independent groups. The independent samples t-test comes in two different forms:

Assumptions

-  Independence of the observations. There is no relationship between the observations in each group.
-  No significant outliers in the groups
-  the two groups of samples should be normally distributed.
-  If using the Student's t-test, the variances of the two groups should not be significantly different. This assumption is relaxed in the Welch’s t-test.

#### Example: Gentoo vs. Adelie body mass 

Let's use the independent sample t-test to see if there is a significant difference in the mean body mass of Gentoo penguins vs. Adelie penguins:

```{r}
# Simplify the dataset to what we need
data_for_t_test = penguins %>%
  filter(species %in% c("Gentoo", "Adelie"),
         !is.na(body_mass_g)) %>%
  select(species, body_mass_g) %>%
  droplevels() # This removes the "Chinstrap" level from the species factor

# Calculate summary stats
data_for_t_test %>%
  group_by(species) %>%
  summarize(mean=mean(body_mass_g), sd=sd(body_mass_g))

# Plot a quick histogram:
ggplot(aes(x=body_mass_g), data=data_for_t_test) +
  geom_histogram() +
  facet_wrap(~species)

# Look for the presence of outliers
data_for_t_test %>%
  group_by(species) %>%
  identify_outliers(body_mass_g)

# Check normality assumption with a qqplot:
ggplot(data_for_t_test) +
  stat_qq(aes(sample=body_mass_g)) +
  facet_wrap(~species)

# Check equality of variances
data_for_t_test %>% levene_test(body_mass_g ~ species)
```

We are examining the distribution of body mass observations for Gentoo and Adelie penguins, and the histograms looked normal. No outliers were found. The Q-Q plots looked fine - the data fall along the 1:1 line. Levene's test checks for equality of variances, and since p>0.05, we accept the null hypothesis that the variances are equal. That means we can use the Student's t-test if we want (we'll probably be "safe" and use Welch's t-test anyway). Whew. Stats are exhuasting, right?

Now let's actually run the independent sample t-test.

```{r}
# Base R version:
t.test(data_for_t_test$body_mass_g ~ data_for_t_test$species)

# dplyr-friendly version:
data_for_t_test %>% 
  t_test(body_mass_g ~ species) 

# Calculate the effect size:
data_for_t_test %>%  cohens_d(body_mass_g ~ species)
```

So we can reject the null hypothesis that the means are equal, and accept the alternative hypothesis. Adelie body mass is significantly lower than Gentoo body mass (Welch's t-test, p<0.001). 

If we had wanted to run a Student's t-test, we could have just included a parameter in the t.test function `var.equal=TRUE`. I'm also showing both the base R `t.test()` and the dplyr-friendly `t_test` from the `rstatix` package. A nice thing about `t.test()` is that there is a lot of hand-holding in the results output (who doesn't love having their hand held in statistics?). An advantage of `t_test()` is that the output is in a simple table, so tranferring these results into a report is easier, especially if you have a bunch of t-tests to do and you can build a data frame with a new test result in each row.

### Paired sample t-test:

The paired sample t-test is used to compare the means of two related groups of samples. Put into another words, it’s used in a situation where you have two pairs of values measured for the same samples. For example, you might want to compare the average weight of 20 sea urchins before and after some experimental treatment. The paired t-test can be used to compare the mean weights before and after treatment.

Assumptions:

-  No significant outliers in the differences between groups
-  The difference of pairs should follow a normal distribution.

We don't have any good exmaples of paired data in our penguins dataset. However, conducting the paired t-test looks almost identical to the independent sample t-test, except that you test your assumptions for outliers and normality on the difference between the paired data (i.e. for each individual urchin, the difference is the weight before treatment minus the weight after treatment). Then run `t_test()` with the parameter `paired=TRUE`.

### Note on assumptions:

**Assessing normality:** With large enough samples size (n > 30) the violation of the normality assumption should not cause major problems (according to the central limit theorem). This implies that we can ignore the distribution of the data and use parametric tests. However, to be consistent, the Shapiro-Wilk test can be used to ascertain whether data show or not a serious deviation from normality (See Chapter @ref(normality-test-in-r)).

**Assessing equality of variances:** Homogeneity of variances can be checked using the Levene’s test. Note that, by default, the t_test() function does not assume equal variances; instead of the standard Student’s t-test, it uses the Welch t-test by default, which is the considered the safer one. To use Student’s t-test, set var.equal = TRUE. The two methods give very similar results unless both the group sizes and the standard deviations are very different.

In the situations where the assumptions are violated, non-parametric tests, such as Wilcoxon test, are recommended.


### Acknowledgements

Although we used our beloved penguin data, the structure of this lesson was inspired by Data Novia:

https://www.datanovia.com/en/lessons/t-test-in-r/

