---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, collapse=TRUE) 
```

### Unit 3: Penguins
#### Lesson 5: Linear models
#### New functions: lm(), geom_smooth(), ( predict(), AIC(), step(), ...?)

***

### Linear regression

Linear regression is one of the most commonly-used and easy-to-understand approaches to modeling. Linear regression involves a numerical outcome variable y and explanatory variable(s) x that are either numerical or categorical. We will start with simple (univariate) linear regression where y is modeled as a function of a single x variable.

The mathematical equation for simple regression is as follows:
$$y = \beta_1 + \beta_2*x + \varepsilon$$
where $\beta_1$ is the y-intercept, $\beta_2$ is the slope and $\varepsilon$ is the error term, or the portion of y that the regression model can't explain. 

### Model assumptions

Linear regression has 5 key assumptions:

-  Linear relationship 
-  Multivariate normality
-  No or little multicollinearity
-  No auto-correlation (samples are independent)
-  Homoscedasticity (variance is equal along the regression line)

Additionally, a good sample size rule of thumb is that the regression analysis requires at least 20 cases per independent variable in the analysis.

Here are some plots you want to look at before building a linear regression model:

-  Scatter plot: Visualise the linear relationship between the predictor and response, check for auto-correlation (if needed) and heteroscedasticity
-  Box plot: To spot any outlier observations in the variable. Having outliers in your predictor can drastically affect the predictions as they can affect the direction/slope of the line of best fit.
-  Histogram / density plot: To see the distribution of the predictor variable. Ideally, a close to normal distribution (a bell shaped curve), without being skewed to the left or right is preferred.
-  Q-Q plot: Also good for checking normality of your predictor variable
-  Correlation matrix (like `GGally::ggpairs()`) to check for multicollinearity in your explanatory variables

If you do encounter some problems with your data, there are many solutions that can help make linear regression an appropriate analysis. For example, if your explanatory variables aren't normal or you have heterscedasticity, a nonlinear transformation (such as $log(x)$, $x^2$ or $\sqrt{x}$) may solve the issue. If you have some nasty outliers, think about whether you might be (scientifically) justified in removing them from the dataset. If you several of your explanatory variables are correlated, you can remove some of them using stepwise regression. These will be covered in detail in a statistics class.

### Simple linear regression

Let's use the `palmerpenguins` data to look again at the relationship between bill length and bill depth. I'll do a little exploratory data analysis by viewing the first few data points and calculating summary statistics, then I'll look at the density plots and correlation with `GGally::ggpairs()`

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(palmerpenguins)
library(GGally)

# Exploratory data analysis:
glimpse(penguins)
summarize(penguins)
penguins %>% 
  select(bill_depth_mm, bill_length_mm) %>%
  GGally::ggpairs()
```

The linearity of the explanatory variable `bill_depth_mm` and the independent variable `bill_depth_mm` doesn't look very promising to me. Let's see what happens if we actually build the linear regression. We'll use the function `lm()` which stands for "linear model" and we'll indicate the relationship we want to test by putting a formula of the format `y~x` as a parameter. We'll save the model results in a variable, then use the `summary()` function to display the model results: 

```{r}
lm_1 = lm(bill_depth_mm ~ bill_length_mm, data=penguins)
summary(lm_1)
```

Both coefficients $\beta_1$ (the y-intercept) and $\beta_2$ (the slope associated with bill length) are statistically significant with $p<0.05$. However, the R-squared = 0.05, which means that bill length only explains about 5% of the variance in the bill depth observations. That's pretty pathetic. Since this is just a simple univariate regression model we can quickly plot the data and the linear model using `geom_smooth()` with `method="lm"` in `ggplot.` 

```{r}
ggplot(data=penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +
     geom_point() +
     geom_smooth(method = "lm")
```

There are clear clusters in the data that are being ignored by our regression model, and the line doesn't seem to capture any interesting trend. The negative coefficient for bill_length_mm (as well as the plotted model) indicates that as bill length increases, bill depth decreases. That doesn't really make sense, right? What did we do wrong?

Well, grouping all of the penguin data, i.e. all three species, is pretty illogical. In fact, it violates another, less cited, assumption of linear regression: "All necessary independent variables are included in the regression that are specified by existing theory and/or research." We probably should have included species, or separated our analysis out into three separate models, one for each species. 

To check some of the assumptions of your linear model *post hoc*, you can send your saved model to the plot() function in base R:

```{r}
class(lm_1) # Note that your model output is a variable of the class "lm"
plot(lm_1)  # This actually calls plot.lm() since the first parameter is class "lm"
```

You can learn more about this quick-and-dirty diagnostics plotting trick by looking up the help page `?plot.lm`. The function `plot.lm()` is the version of the `plot()` function that is called when the parameter that you pass `plot()` is of the class "lm". This output provides you four useful plots:

-  Residuals vs Fitted Values, to check constant variance in residuals and linearity of association between predictors and outcome (look for a relatively straight line and random-looking scatterplot). By default, the 3 points with the highest residuals are labeled (i.e. the row number is printed on the figure).
-  Normal Q-Q Plot, to check the assumption of normally distributed residuals.
-  Root of Standardized residuals vs Fitted values, this is very similar to number 1, where the Y axis of residuals is in a different metric.
-  Residuals vs Leverage, to check if the leverage of certain observations are driving abnormal residual distributions, thus violating assumptions and biasing statistical tests. 

There are many objective statistical tests that can be performed to check the assumptions of your data. For more resources, look at the end of this tutorial page.

### Another simple linear model

Let's do the logical thing and test the same relationship `bill_depth_mm ~ bill_length_mm` but only looking at one species:

```{r}
gentoo = penguins %>% filter(species=="Gentoo")
lm_2 = lm(bill_depth_mm ~ bill_length_mm, data=gentoo)
summary(lm_2)
ggplot(data=gentoo, aes(x = bill_length_mm, y = bill_depth_mm)) +
     geom_point() +
     geom_smooth(method = "lm")
```

This trend looks a little better to me. The R-squared = 0.41, so the model explains about 41% of the variation. That's pretty impressive considering we only have one variable in there. Also, the model passes the sanity check because it seems logical that bill depth will increase with increasing bill length. 

We can actually use geom_smooth() to examine separate linear models for each of the three penguin species at once without formally running the regression:

```{r}
ggplot(data=penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=species)) +
     geom_point() +
     geom_smooth(method = "lm")
```

So that makes it very clear that running all three species together was a serious logical error, and it produced a completely untrue result that bill depth increases as bill length decreases In fact, when we split the three species apart, we can see that bill depth increases with increasing bill length (as you'd expect) and the relationships look pretty similar for each species. This is an example of Simpson's paradox, which occurs when trends that appear when a dataset is separated into groups reverse when the data are aggregated. Basically, when doing statistics, you want to use your brain and your gut to build models that make sense.

### Multiple linear regression

In life, we typically can't do a very good job explaining variation in some dependent variable using just a single independent variable. Multiple linear regression is when we build a function with multiple independent variables of the form:

$$y = \beta_0 + \beta_1*x_1 + \beta_2*x_2 + ... + \varepsilon$$

When conducting multiple linear regression, all of the same assumptions and diagnostics apply, with one important addition: the interpretation of the associated effect of any one explanatory variable must be made in conjunction with the other explanatory variables included in your model.

#### A note on model goals:

The statistical decisions you make should account for your end goals. These are the two types of goals when building a model:

1. *Modeling for explanation:* When you want to explicitly describe and quantify the relationship between the outcome variable y and a set of explanatory variables x, determine the significance of any relationships, have measures summarizing these relationships, and possibly identify any causal relationships between the variables.
2. *Modeling for prediction:* When you want to predict an outcome variable  y based on the information contained in a set of predictor variables x. Unlike modeling for explanation, however, you donâ€™t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about y using the information in x.

In this lesson, and in general when you are trying to "play it safe" in your analyses, you will be modeling for explanation. This means if you are modeling some $y$ as a a function of $x_1$ and $x_2$, but $x_1$ and $x_2$ are quite collinear, then you won't be able to differentiate the unique impact of either $x$ variable on $y$ because, for example, $x_1$ may be stealing some of the variation from $x_2$. Then the total impact of $x_2$ on $y$ will be unfairly biased small, and the impact of $x_1$ will be unfairly biased large. Who knows what chaos will ensue when you use these biased models to guide science, policy, etc.?

However, if you are modeling for prediction, and don't actually care what the relative impact of $x_1$ or $x_2$ is on $y$, but you want your $y$ predictions to be as accurate and precise as possible, then you don't have to worry about multicollinearity. For example, weather predictions are like this. Meteorologists just throw everything they can into their models, even though their "explanatory variables" can be highly correlated, becuase they aren't trying to demonstrate a relationship between rainfall and pressure, they are just trying to let you know whether it'll be a good weekend for a beach trip.

#### Continuous vs. categorical variables:

-  Continuous variable: numeric variables that can have an infinite number of values between any two values. Example: `bill_depth_mm`, `bill_length_mm`
-  Categorical varialbe: (a.k.a. discrete or nominal variables) contain a finite number of categories or distinct groups. Example: `species`. These should typically be in the `factor` class in R.

There are cases when you, the data scientist, can make a conscientious choice between assigning a variable as continuous vs. categorical. A good example is "year". If you are looking for a trend in your data over time, year should be treated as a continuous variable. If you are trying to account for some wacky conditions that can change from year to year, but that probably don't consitute a temporal trend, you could treat year as a categorical variable.

#### One continuous and one categorical explanatory variable

Our adventures in simple regression taught us that we shouldn't model bill depth as a function of bill length without accounting for species. We built three separate models, one for each species. Another option is to build one model, but include species as an explanatory variable:

$$bill_depth_mm ~ bill_length_mm + species$$

This is very simple to implement in the lm() function:
```{r}
lm_3 = lm(bill_depth_mm ~ bill_length_mm + species, data=penguins)
summary(lm_3)
```

There are several different ways to access your model results. You can copy and paste coefficient estimates, p-values, etc. from the summary() output, or you can extract various elements from the model variable using specialized functions like coef(). The most efficient way to get at your model results makes use of the `broom` package in the tidyverse.  

```{r}
str(lm_3)
coef(lm_3)
anova(lm_3)
broom::tidy(lm_3)
broom::tidy(lm_3, conf.int = TRUE, conf.level = 0.95)

```



```{r}
library(ggiraph)
library(ggiraphExtra)
ggPredict(lm_3, se=TRUE, interactive=TRUE)
```

Including species:

```{r}

lm_4 = lm(bill_depth_mm ~ bill_length_mm*species, data=penguins)
AIC(lm_1, lm_3, lm_4)

# https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.html
ggPredict(lm_4, se=TRUE, interactive=TRUE) + main("hi")
```

With predict():
https://aosmith.rbind.io/2018/11/16/plot-fitted-lines/
interval = confidence vs. interval = prediction

with ggPredict():
https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.html


### More information

Overview of basic regression capabilities in R:
https://www.statmethods.net/stats/regression.html

Diagnostic tests for linear regression:
https://www.statmethods.net/stats/rdiagnostics.html
https://www.ianruginski.com/post/regressionassumptions/

